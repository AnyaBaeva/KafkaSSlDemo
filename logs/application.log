2025-09-26 00:01:07 [main] INFO  org.apache.spark.SparkContext - Running Spark version 3.5.0
2025-09-26 00:01:07 [main] INFO  org.apache.spark.SparkContext - OS info Windows 10, 10.0, amd64
2025-09-26 00:01:07 [main] INFO  org.apache.spark.SparkContext - Java version 11.0.14.1
2025-09-26 00:01:07 [main] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-09-26 00:01:08 [main] INFO  o.a.spark.resource.ResourceUtils - ==============================================================
2025-09-26 00:01:08 [main] INFO  o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-09-26 00:01:08 [main] INFO  o.a.spark.resource.ResourceUtils - ==============================================================
2025-09-26 00:01:08 [main] INFO  org.apache.spark.SparkContext - Submitted application: HDFS Brand Analysis
2025-09-26 00:01:08 [main] INFO  o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-09-26 00:01:08 [main] INFO  o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-09-26 00:01:08 [main] INFO  o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-09-26 00:01:08 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: Даша,Äàøà
2025-09-26 00:01:08 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: Даша,Äàøà
2025-09-26 00:01:08 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-09-26 00:01:08 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-09-26 00:01:08 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Даша, Äàøà; groups with view permissions: EMPTY; users with modify permissions: Даша, Äàøà; groups with modify permissions: EMPTY
2025-09-26 00:01:09 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 56885.
2025-09-26 00:01:09 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-09-26 00:01:09 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-09-26 00:01:09 [main] INFO  o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-09-26 00:01:09 [main] INFO  o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-09-26 00:01:09 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-09-26 00:01:09 [main] INFO  o.a.spark.storage.DiskBlockManager - Created local directory at C:\Users\Даша\AppData\Local\Temp\blockmgr-8c350c31-e018-4f0b-b6cb-11cc92e21614
2025-09-26 00:01:09 [main] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 2.2 GiB
2025-09-26 00:01:09 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-09-26 00:01:09 [main] INFO  org.sparkproject.jetty.util.log - Logging initialized @5085ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-09-26 00:01:10 [main] INFO  org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-09-26 00:01:10 [main] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 11.0.14.1+10-LTS
2025-09-26 00:01:10 [main] INFO  o.sparkproject.jetty.server.Server - Started @5286ms
2025-09-26 00:01:10 [main] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@1292071f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-09-26 00:01:10 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d1dcdff{/,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host DESKTOP-GVQ7N79
2025-09-26 00:01:10 [main] INFO  org.apache.spark.executor.Executor - OS info Windows 10, 10.0, amd64
2025-09-26 00:01:10 [main] INFO  org.apache.spark.executor.Executor - Java version 11.0.14.1
2025-09-26 00:01:10 [main] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-09-26 00:01:10 [main] INFO  org.apache.spark.executor.Executor - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@77681ce4 for default.
2025-09-26 00:01:10 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56929.
2025-09-26 00:01:10 [main] INFO  o.a.s.n.n.NettyBlockTransferService - Server created on DESKTOP-GVQ7N79:56929
2025-09-26 00:01:10 [main] INFO  o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-09-26 00:01:10 [main] INFO  o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, DESKTOP-GVQ7N79, 56929, None)
2025-09-26 00:01:10 [dispatcher-BlockManagerMaster] INFO  o.a.s.s.BlockManagerMasterEndpoint - Registering block manager DESKTOP-GVQ7N79:56929 with 2.2 GiB RAM, BlockManagerId(driver, DESKTOP-GVQ7N79, 56929, None)
2025-09-26 00:01:10 [main] INFO  o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, DESKTOP-GVQ7N79, 56929, None)
2025-09-26 00:01:10 [main] INFO  o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, DESKTOP-GVQ7N79, 56929, None)
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6d1dcdff{/,null,STOPPED,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@558b4942{/jobs,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@70025b99{/jobs/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40de8f93{/jobs/job,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b55dd15{/jobs/job/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ba3d4b6{/stages,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@441fbe89{/stages/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28a9494b{/stages/stage,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1163a27{/stages/stage/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@502a4156{/stages/pool,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64e1377c{/stages/pool/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@92d1782{/storage,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72976b4{/storage/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1319bc2a{/storage/rdd,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42f85fa4{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27abb6ca{/environment,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6968c1d6{/environment/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77f991c{/executors,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ceee4b6{/executors/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6968bcec{/executors/threadDump,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e784320{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@13c8ac77{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4cad79bc{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a9eccc4{/static,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3f96f020{/,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26ca61bf{/api,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@771a7d53{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e95595b{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-09-26 00:01:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@16ac5d35{/metrics/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:11 [main] WARN  org.apache.spark.SparkContext - Using an existing SparkContext; some configuration may not take effect.
2025-09-26 00:01:11 [main] INFO  o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-09-26 00:01:12 [main] INFO  o.a.spark.sql.internal.SharedState - Warehouse path is 'hdfs://localhost:9000/user/hive/warehouse'.
2025-09-26 00:01:12 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18e4551{/SQL,null,AVAILABLE,@Spark}
2025-09-26 00:01:12 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@11e834ad{/SQL/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:12 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6e068ac9{/SQL/execution,null,AVAILABLE,@Spark}
2025-09-26 00:01:12 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@bb6f3f7{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-09-26 00:01:12 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@639cb788{/static/sql,null,AVAILABLE,@Spark}
2025-09-26 00:01:15 [main] WARN  org.apache.hadoop.fs.FileSystem - Failed to initialize fileystem hdfs://namenode:9000/data/products: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode
2025-09-26 00:01:15 [main] WARN  o.a.s.s.e.streaming.FileStreamSink - Assume no metadata directory. Error while looking for metadata directory in the path: hdfs://namenode:9000/data/products.
java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:466)
	at org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:134)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:374)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initDFSClient(DistributedFileSystem.java:202)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:187)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:646)
	at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:683)
	at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:655)
	at com.example.spark.KafkaHdfsSparkConsumer.analyzeBrand(KafkaHdfsSparkConsumer.java:52)
	at com.example.spark.KafkaHdfsSparkConsumer.main(KafkaHdfsSparkConsumer.java:39)
Caused by: java.net.UnknownHostException: namenode
	... 23 common frames omitted
2025-09-26 00:01:15 [main] WARN  org.apache.hadoop.fs.FileSystem - Failed to initialize fileystem hdfs://namenode:9000/data/products: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode
2025-09-26 00:01:15 [main] INFO  org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-09-26 00:01:15 [main] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@1292071f{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-09-26 00:01:15 [main] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://DESKTOP-GVQ7N79:4040
2025-09-26 00:01:15 [dispatcher-event-loop-7] INFO  o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-09-26 00:01:15 [main] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-09-26 00:01:15 [main] INFO  o.apache.spark.storage.BlockManager - BlockManager stopped
2025-09-26 00:01:15 [main] INFO  o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-09-26 00:01:15 [dispatcher-event-loop-1] INFO  o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-09-26 00:01:15 [main] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-09-26 00:01:15 [shutdown-hook-0] INFO  o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-09-26 00:01:15 [shutdown-hook-0] INFO  o.a.spark.util.ShutdownHookManager - Deleting directory C:\Users\Даша\AppData\Local\Temp\spark-fe21be72-064c-41a4-8059-9a1621d59fc7
2025-09-26 00:02:11 [main] INFO  org.apache.spark.SparkContext - Running Spark version 3.5.0
2025-09-26 00:02:11 [main] INFO  org.apache.spark.SparkContext - OS info Windows 10, 10.0, amd64
2025-09-26 00:02:11 [main] INFO  org.apache.spark.SparkContext - Java version 11.0.14.1
2025-09-26 00:02:11 [main] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-09-26 00:02:12 [main] INFO  o.a.spark.resource.ResourceUtils - ==============================================================
2025-09-26 00:02:12 [main] INFO  o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-09-26 00:02:12 [main] INFO  o.a.spark.resource.ResourceUtils - ==============================================================
2025-09-26 00:02:12 [main] INFO  org.apache.spark.SparkContext - Submitted application: HDFS Brand Analysis
2025-09-26 00:02:12 [main] INFO  o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-09-26 00:02:12 [main] INFO  o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-09-26 00:02:12 [main] INFO  o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-09-26 00:02:12 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: Даша,Äàøà
2025-09-26 00:02:12 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: Даша,Äàøà
2025-09-26 00:02:12 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-09-26 00:02:12 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-09-26 00:02:12 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Даша, Äàøà; groups with view permissions: EMPTY; users with modify permissions: Даша, Äàøà; groups with modify permissions: EMPTY
2025-09-26 00:02:13 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 57009.
2025-09-26 00:02:13 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-09-26 00:02:13 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-09-26 00:02:13 [main] INFO  o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-09-26 00:02:13 [main] INFO  o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-09-26 00:02:13 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-09-26 00:02:13 [main] INFO  o.a.spark.storage.DiskBlockManager - Created local directory at C:\Users\Даша\AppData\Local\Temp\blockmgr-8de0104d-b732-40be-918c-89110a8654dc
2025-09-26 00:02:13 [main] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 2.2 GiB
2025-09-26 00:02:13 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-09-26 00:02:13 [main] INFO  org.sparkproject.jetty.util.log - Logging initialized @4322ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-09-26 00:02:13 [main] INFO  org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-09-26 00:02:13 [main] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 11.0.14.1+10-LTS
2025-09-26 00:02:13 [main] INFO  o.sparkproject.jetty.server.Server - Started @4494ms
2025-09-26 00:02:14 [main] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@53ec2968{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-09-26 00:02:14 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66273da0{/,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host DESKTOP-GVQ7N79
2025-09-26 00:02:14 [main] INFO  org.apache.spark.executor.Executor - OS info Windows 10, 10.0, amd64
2025-09-26 00:02:14 [main] INFO  org.apache.spark.executor.Executor - Java version 11.0.14.1
2025-09-26 00:02:14 [main] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-09-26 00:02:14 [main] INFO  org.apache.spark.executor.Executor - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@78e17a99 for default.
2025-09-26 00:02:14 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57053.
2025-09-26 00:02:14 [main] INFO  o.a.s.n.n.NettyBlockTransferService - Server created on DESKTOP-GVQ7N79:57053
2025-09-26 00:02:14 [main] INFO  o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-09-26 00:02:14 [main] INFO  o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, DESKTOP-GVQ7N79, 57053, None)
2025-09-26 00:02:14 [dispatcher-BlockManagerMaster] INFO  o.a.s.s.BlockManagerMasterEndpoint - Registering block manager DESKTOP-GVQ7N79:57053 with 2.2 GiB RAM, BlockManagerId(driver, DESKTOP-GVQ7N79, 57053, None)
2025-09-26 00:02:14 [main] INFO  o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, DESKTOP-GVQ7N79, 57053, None)
2025-09-26 00:02:14 [main] INFO  o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, DESKTOP-GVQ7N79, 57053, None)
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@66273da0{/,null,STOPPED,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6413d7e7{/jobs,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17b37e9a{/jobs/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@250a9031{/jobs/job,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d67f5eb{/jobs/job/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1fd62b{/stages,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@558b4942{/stages/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40de8f93{/stages/stage,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b55dd15{/stages/stage/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ba3d4b6{/stages/pool,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@441fbe89{/stages/pool/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45ab3bdd{/storage,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1642eeae{/storage/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28a9494b{/storage/rdd,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1163a27{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@502a4156{/environment,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64e1377c{/environment/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@92d1782{/executors,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72976b4{/executors/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1319bc2a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42f85fa4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27abb6ca{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6968c1d6{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77f991c{/static,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60e3c26e{/,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@999b951{/api,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fa5f81c{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2f0109{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40c2ce52{/metrics/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:14 [main] WARN  org.apache.spark.SparkContext - Using an existing SparkContext; some configuration may not take effect.
2025-09-26 00:02:15 [main] INFO  o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-09-26 00:02:15 [main] INFO  o.a.spark.sql.internal.SharedState - Warehouse path is 'hdfs://localhost:9000/user/hive/warehouse'.
2025-09-26 00:02:15 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45dde6ac{/SQL,null,AVAILABLE,@Spark}
2025-09-26 00:02:15 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@892af0e{/SQL/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:15 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c8a445b{/SQL/execution,null,AVAILABLE,@Spark}
2025-09-26 00:02:15 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37b1149b{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-09-26 00:02:15 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d911a{/static/sql,null,AVAILABLE,@Spark}
2025-09-26 00:02:19 [main] WARN  org.apache.hadoop.fs.FileSystem - Failed to initialize fileystem hdfs://namenode:9000/data/products/*: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode
2025-09-26 00:02:19 [main] WARN  o.a.s.s.e.streaming.FileStreamSink - Assume no metadata directory. Error while looking for metadata directory in the path: hdfs://namenode:9000/data/products/*.
java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:466)
	at org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:134)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:374)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initDFSClient(DistributedFileSystem.java:202)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:187)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:646)
	at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:683)
	at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:655)
	at com.example.spark.KafkaHdfsSparkConsumer.analyzeBrand(KafkaHdfsSparkConsumer.java:52)
	at com.example.spark.KafkaHdfsSparkConsumer.main(KafkaHdfsSparkConsumer.java:39)
Caused by: java.net.UnknownHostException: namenode
	... 23 common frames omitted
2025-09-26 00:02:19 [main] WARN  org.apache.hadoop.fs.FileSystem - Failed to initialize fileystem hdfs://namenode:9000/data/products/*: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode
2025-09-26 00:02:19 [main] INFO  org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-09-26 00:02:19 [main] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@53ec2968{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-09-26 00:02:19 [main] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://DESKTOP-GVQ7N79:4040
2025-09-26 00:02:19 [dispatcher-event-loop-7] INFO  o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-09-26 00:02:19 [main] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-09-26 00:02:19 [main] INFO  o.apache.spark.storage.BlockManager - BlockManager stopped
2025-09-26 00:02:19 [main] INFO  o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-09-26 00:02:19 [dispatcher-event-loop-1] INFO  o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-09-26 00:02:19 [main] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-09-26 00:02:19 [shutdown-hook-0] INFO  o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-09-26 00:02:19 [shutdown-hook-0] INFO  o.a.spark.util.ShutdownHookManager - Deleting directory C:\Users\Даша\AppData\Local\Temp\spark-1aa40788-38e8-42f6-a6b5-8183fac9dbcd
2025-09-26 00:06:07 [main] INFO  org.apache.spark.SparkContext - Running Spark version 3.5.0
2025-09-26 00:06:07 [main] INFO  org.apache.spark.SparkContext - OS info Windows 10, 10.0, amd64
2025-09-26 00:06:07 [main] INFO  org.apache.spark.SparkContext - Java version 11.0.14.1
2025-09-26 00:06:07 [main] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-09-26 00:06:07 [main] INFO  o.a.spark.resource.ResourceUtils - ==============================================================
2025-09-26 00:06:07 [main] INFO  o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-09-26 00:06:07 [main] INFO  o.a.spark.resource.ResourceUtils - ==============================================================
2025-09-26 00:06:07 [main] INFO  org.apache.spark.SparkContext - Submitted application: HDFS Brand Analysis
2025-09-26 00:06:07 [main] INFO  o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-09-26 00:06:08 [main] INFO  o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-09-26 00:06:08 [main] INFO  o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-09-26 00:06:08 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: Даша,Äàøà
2025-09-26 00:06:08 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: Даша,Äàøà
2025-09-26 00:06:08 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-09-26 00:06:08 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-09-26 00:06:08 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Даша, Äàøà; groups with view permissions: EMPTY; users with modify permissions: Даша, Äàøà; groups with modify permissions: EMPTY
2025-09-26 00:06:09 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 57248.
2025-09-26 00:06:09 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-09-26 00:06:09 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-09-26 00:06:09 [main] INFO  o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-09-26 00:06:09 [main] INFO  o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-09-26 00:06:09 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-09-26 00:06:09 [main] INFO  o.a.spark.storage.DiskBlockManager - Created local directory at C:\Users\Даша\AppData\Local\Temp\blockmgr-99d9841c-2cd6-41b0-9700-2887378fe68f
2025-09-26 00:06:09 [main] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 2.2 GiB
2025-09-26 00:06:09 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-09-26 00:06:09 [main] INFO  org.sparkproject.jetty.util.log - Logging initialized @4147ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-09-26 00:06:09 [main] INFO  org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-09-26 00:06:09 [main] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 11.0.14.1+10-LTS
2025-09-26 00:06:09 [main] INFO  o.sparkproject.jetty.server.Server - Started @4331ms
2025-09-26 00:06:09 [main] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@53ec2968{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-09-26 00:06:09 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-09-26 00:06:09 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66273da0{/,null,AVAILABLE,@Spark}
2025-09-26 00:06:09 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host DESKTOP-GVQ7N79
2025-09-26 00:06:09 [main] INFO  org.apache.spark.executor.Executor - OS info Windows 10, 10.0, amd64
2025-09-26 00:06:09 [main] INFO  org.apache.spark.executor.Executor - Java version 11.0.14.1
2025-09-26 00:06:09 [main] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-09-26 00:06:09 [main] INFO  org.apache.spark.executor.Executor - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@78e17a99 for default.
2025-09-26 00:06:10 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57291.
2025-09-26 00:06:10 [main] INFO  o.a.s.n.n.NettyBlockTransferService - Server created on DESKTOP-GVQ7N79:57291
2025-09-26 00:06:10 [main] INFO  o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-09-26 00:06:10 [main] INFO  o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, DESKTOP-GVQ7N79, 57291, None)
2025-09-26 00:06:10 [dispatcher-BlockManagerMaster] INFO  o.a.s.s.BlockManagerMasterEndpoint - Registering block manager DESKTOP-GVQ7N79:57291 with 2.2 GiB RAM, BlockManagerId(driver, DESKTOP-GVQ7N79, 57291, None)
2025-09-26 00:06:10 [main] INFO  o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, DESKTOP-GVQ7N79, 57291, None)
2025-09-26 00:06:10 [main] INFO  o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, DESKTOP-GVQ7N79, 57291, None)
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@66273da0{/,null,STOPPED,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6413d7e7{/jobs,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17b37e9a{/jobs/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@250a9031{/jobs/job,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d67f5eb{/jobs/job/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1fd62b{/stages,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@558b4942{/stages/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40de8f93{/stages/stage,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b55dd15{/stages/stage/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ba3d4b6{/stages/pool,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@441fbe89{/stages/pool/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45ab3bdd{/storage,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1642eeae{/storage/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28a9494b{/storage/rdd,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1163a27{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@502a4156{/environment,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64e1377c{/environment/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@92d1782{/executors,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72976b4{/executors/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1319bc2a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42f85fa4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27abb6ca{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6968c1d6{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77f991c{/static,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60e3c26e{/,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@999b951{/api,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fa5f81c{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2f0109{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40c2ce52{/metrics/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:10 [main] WARN  org.apache.spark.SparkContext - Using an existing SparkContext; some configuration may not take effect.
2025-09-26 00:06:10 [main] INFO  o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-09-26 00:06:11 [main] INFO  o.a.spark.sql.internal.SharedState - Warehouse path is 'hdfs://localhost:9000/user/hive/warehouse'.
2025-09-26 00:06:11 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45dde6ac{/SQL,null,AVAILABLE,@Spark}
2025-09-26 00:06:11 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@892af0e{/SQL/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:11 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2c8a445b{/SQL/execution,null,AVAILABLE,@Spark}
2025-09-26 00:06:11 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@37b1149b{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-09-26 00:06:11 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@25d911a{/static/sql,null,AVAILABLE,@Spark}
2025-09-26 00:06:42 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 0 time(s); maxRetries=45
2025-09-26 00:07:02 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 1 time(s); maxRetries=45
2025-09-26 00:07:22 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 2 time(s); maxRetries=45
2025-09-26 00:07:42 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 3 time(s); maxRetries=45
2025-09-26 00:08:02 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 4 time(s); maxRetries=45
2025-09-26 00:08:22 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 5 time(s); maxRetries=45
2025-09-26 00:08:42 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 6 time(s); maxRetries=45
2025-09-26 00:09:02 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 7 time(s); maxRetries=45
2025-09-26 00:09:22 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 8 time(s); maxRetries=45
2025-09-26 00:09:42 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 9 time(s); maxRetries=45
2025-09-26 00:10:02 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 10 time(s); maxRetries=45
2025-09-26 00:10:22 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 11 time(s); maxRetries=45
2025-09-26 00:10:31 [shutdown-hook-0] INFO  org.apache.spark.SparkContext - Invoking stop() from shutdown hook
2025-09-26 00:10:31 [shutdown-hook-0] INFO  org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-09-26 00:10:31 [shutdown-hook-0] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@53ec2968{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-09-26 00:10:31 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://DESKTOP-GVQ7N79:4040
2025-09-26 00:10:31 [dispatcher-event-loop-0] INFO  o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-09-26 00:10:31 [shutdown-hook-0] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-09-26 00:10:31 [shutdown-hook-0] INFO  o.apache.spark.storage.BlockManager - BlockManager stopped
2025-09-26 00:10:31 [shutdown-hook-0] INFO  o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-09-26 00:10:31 [dispatcher-event-loop-5] INFO  o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-09-26 00:10:31 [shutdown-hook-0] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-09-26 00:10:31 [shutdown-hook-0] INFO  o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-09-26 00:10:31 [shutdown-hook-0] INFO  o.a.spark.util.ShutdownHookManager - Deleting directory C:\Users\Даша\AppData\Local\Temp\spark-b55f91a3-2672-4f09-b745-8203ae3ae70b
2025-09-26 00:10:31 [main] WARN  o.a.s.s.e.streaming.FileStreamSink - Assume no metadata directory. Error while looking for metadata directory in the path: hdfs://172.19.0.12:9000/data/products/*.
java.io.InterruptedIOException: DestHost:destPort 172.19.0.12:9000 , LocalHost:localPort DESKTOP-GVQ7N79/192.168.1.34:0. Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connection-pending remote=172.19.0.12/172.19.0.12:9000]. Total timeout mills is 20000, 11065 millis timeout left.
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:888)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy21.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy22.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:646)
	at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:683)
	at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:655)
	at com.example.spark.KafkaHdfsSparkConsumer.analyzeBrand(KafkaHdfsSparkConsumer.java:52)
	at com.example.spark.KafkaHdfsSparkConsumer.main(KafkaHdfsSparkConsumer.java:39)
Caused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connection-pending remote=172.19.0.12/172.19.0.12:9000]. Total timeout mills is 20000, 11065 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:350)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:202)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	... 32 common frames omitted
2025-09-26 00:10:31 [main] INFO  org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-09-26 00:11:55 [main] INFO  org.apache.spark.SparkContext - Running Spark version 3.5.0
2025-09-26 00:11:55 [main] INFO  org.apache.spark.SparkContext - OS info Windows 10, 10.0, amd64
2025-09-26 00:11:55 [main] INFO  org.apache.spark.SparkContext - Java version 11.0.14.1
2025-09-26 00:11:56 [main] WARN  o.a.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-09-26 00:11:56 [main] INFO  o.a.spark.resource.ResourceUtils - ==============================================================
2025-09-26 00:11:56 [main] INFO  o.a.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
2025-09-26 00:11:56 [main] INFO  o.a.spark.resource.ResourceUtils - ==============================================================
2025-09-26 00:11:56 [main] INFO  org.apache.spark.SparkContext - Submitted application: HDFS Brand Analysis
2025-09-26 00:11:56 [main] INFO  o.a.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-09-26 00:11:56 [main] INFO  o.a.spark.resource.ResourceProfile - Limiting resource is cpu
2025-09-26 00:11:56 [main] INFO  o.a.s.r.ResourceProfileManager - Added ResourceProfile id: 0
2025-09-26 00:11:56 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: Даша,Äàøà
2025-09-26 00:11:56 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls to: Даша,Äàøà
2025-09-26 00:11:56 [main] INFO  org.apache.spark.SecurityManager - Changing view acls groups to: 
2025-09-26 00:11:56 [main] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to: 
2025-09-26 00:11:56 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Даша, Äàøà; groups with view permissions: EMPTY; users with modify permissions: Даша, Äàøà; groups with modify permissions: EMPTY
2025-09-26 00:11:57 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 57611.
2025-09-26 00:11:57 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2025-09-26 00:11:57 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2025-09-26 00:11:57 [main] INFO  o.a.s.s.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-09-26 00:11:57 [main] INFO  o.a.s.s.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
2025-09-26 00:11:57 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
2025-09-26 00:11:57 [main] INFO  o.a.spark.storage.DiskBlockManager - Created local directory at C:\Users\Даша\AppData\Local\Temp\blockmgr-74846d9f-981c-44bf-983a-8ea57f292856
2025-09-26 00:11:57 [main] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore started with capacity 2.2 GiB
2025-09-26 00:11:57 [main] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
2025-09-26 00:11:57 [main] INFO  org.sparkproject.jetty.util.log - Logging initialized @5253ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-09-26 00:11:58 [main] INFO  org.apache.spark.ui.JettyUtils - Start Jetty 0.0.0.0:4040 for SparkUI
2025-09-26 00:11:58 [main] INFO  o.sparkproject.jetty.server.Server - jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 11.0.14.1+10-LTS
2025-09-26 00:11:58 [main] INFO  o.sparkproject.jetty.server.Server - Started @5467ms
2025-09-26 00:11:58 [main] INFO  o.s.jetty.server.AbstractConnector - Started ServerConnector@53ec2968{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-09-26 00:11:58 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66273da0{/,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host DESKTOP-GVQ7N79
2025-09-26 00:11:58 [main] INFO  org.apache.spark.executor.Executor - OS info Windows 10, 10.0, amd64
2025-09-26 00:11:58 [main] INFO  org.apache.spark.executor.Executor - Java version 11.0.14.1
2025-09-26 00:11:58 [main] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
2025-09-26 00:11:58 [main] INFO  org.apache.spark.executor.Executor - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@78e17a99 for default.
2025-09-26 00:11:58 [main] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57654.
2025-09-26 00:11:58 [main] INFO  o.a.s.n.n.NettyBlockTransferService - Server created on DESKTOP-GVQ7N79:57654
2025-09-26 00:11:58 [main] INFO  o.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-09-26 00:11:58 [main] INFO  o.a.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, DESKTOP-GVQ7N79, 57654, None)
2025-09-26 00:11:58 [dispatcher-BlockManagerMaster] INFO  o.a.s.s.BlockManagerMasterEndpoint - Registering block manager DESKTOP-GVQ7N79:57654 with 2.2 GiB RAM, BlockManagerId(driver, DESKTOP-GVQ7N79, 57654, None)
2025-09-26 00:11:58 [main] INFO  o.a.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, DESKTOP-GVQ7N79, 57654, None)
2025-09-26 00:11:58 [main] INFO  o.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, DESKTOP-GVQ7N79, 57654, None)
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@66273da0{/,null,STOPPED,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6413d7e7{/jobs,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17b37e9a{/jobs/json,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@250a9031{/jobs/job,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d67f5eb{/jobs/job/json,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3e1fd62b{/stages,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@558b4942{/stages/json,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40de8f93{/stages/stage,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b55dd15{/stages/stage/json,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ba3d4b6{/stages/pool,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@441fbe89{/stages/pool/json,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45ab3bdd{/storage,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1642eeae{/storage/json,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28a9494b{/storage/rdd,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1163a27{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@502a4156{/environment,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64e1377c{/environment/json,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@92d1782{/executors,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@72976b4{/executors/json,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1319bc2a{/executors/threadDump,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@42f85fa4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27abb6ca{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6968c1d6{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@77f991c{/static,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@60e3c26e{/,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@999b951{/api,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@fa5f81c{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2f0109{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@40c2ce52{/metrics/json,null,AVAILABLE,@Spark}
2025-09-26 00:11:58 [main] WARN  org.apache.spark.SparkContext - Using an existing SparkContext; some configuration may not take effect.
2025-09-26 00:11:59 [main] INFO  o.a.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-09-26 00:12:04 [main] INFO  o.a.spark.sql.internal.SharedState - Warehouse path is 'hdfs://172.19.0.12:9000/user/hive/warehouse'.
2025-09-26 00:12:04 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@14ed7ddf{/SQL,null,AVAILABLE,@Spark}
2025-09-26 00:12:04 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@446c8c72{/SQL/json,null,AVAILABLE,@Spark}
2025-09-26 00:12:04 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@356b6b5d{/SQL/execution,null,AVAILABLE,@Spark}
2025-09-26 00:12:04 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1187dc82{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-09-26 00:12:04 [main] INFO  o.s.j.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@18026052{/static/sql,null,AVAILABLE,@Spark}
2025-09-26 00:12:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 0 time(s); maxRetries=45
2025-09-26 00:12:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 1 time(s); maxRetries=45
2025-09-26 00:13:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 2 time(s); maxRetries=45
2025-09-26 00:13:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 3 time(s); maxRetries=45
2025-09-26 00:13:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 4 time(s); maxRetries=45
2025-09-26 00:14:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 5 time(s); maxRetries=45
2025-09-26 00:14:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 6 time(s); maxRetries=45
2025-09-26 00:14:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 7 time(s); maxRetries=45
2025-09-26 00:15:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 8 time(s); maxRetries=45
2025-09-26 00:15:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 9 time(s); maxRetries=45
2025-09-26 00:15:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 10 time(s); maxRetries=45
2025-09-26 00:16:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 11 time(s); maxRetries=45
2025-09-26 00:16:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 12 time(s); maxRetries=45
2025-09-26 00:16:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 13 time(s); maxRetries=45
2025-09-26 00:17:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 14 time(s); maxRetries=45
2025-09-26 00:17:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 15 time(s); maxRetries=45
2025-09-26 00:17:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 16 time(s); maxRetries=45
2025-09-26 00:18:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 17 time(s); maxRetries=45
2025-09-26 00:18:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 18 time(s); maxRetries=45
2025-09-26 00:18:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 19 time(s); maxRetries=45
2025-09-26 00:19:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 20 time(s); maxRetries=45
2025-09-26 00:19:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 21 time(s); maxRetries=45
2025-09-26 00:19:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 22 time(s); maxRetries=45
2025-09-26 00:20:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 23 time(s); maxRetries=45
2025-09-26 00:20:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 24 time(s); maxRetries=45
2025-09-26 00:20:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 25 time(s); maxRetries=45
2025-09-26 00:21:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 26 time(s); maxRetries=45
2025-09-26 00:21:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 27 time(s); maxRetries=45
2025-09-26 00:21:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 28 time(s); maxRetries=45
2025-09-26 00:22:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 29 time(s); maxRetries=45
2025-09-26 00:22:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 30 time(s); maxRetries=45
2025-09-26 00:22:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 31 time(s); maxRetries=45
2025-09-26 00:23:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 32 time(s); maxRetries=45
2025-09-26 00:23:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 33 time(s); maxRetries=45
2025-09-26 00:23:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 34 time(s); maxRetries=45
2025-09-26 00:24:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 35 time(s); maxRetries=45
2025-09-26 00:24:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 36 time(s); maxRetries=45
2025-09-26 00:24:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 37 time(s); maxRetries=45
2025-09-26 00:25:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 38 time(s); maxRetries=45
2025-09-26 00:25:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 39 time(s); maxRetries=45
2025-09-26 00:25:50 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 40 time(s); maxRetries=45
2025-09-26 00:26:10 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 41 time(s); maxRetries=45
2025-09-26 00:26:30 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 42 time(s); maxRetries=45
2025-09-26 00:26:51 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 43 time(s); maxRetries=45
2025-09-26 00:27:11 [main] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 44 time(s); maxRetries=45
2025-09-26 00:27:31 [main] WARN  o.a.s.s.e.streaming.FileStreamSink - Assume no metadata directory. Error while looking for metadata directory in the path: hdfs://172.19.0.12:9000/data/products.
org.apache.hadoop.net.ConnectTimeoutException: Call From DESKTOP-GVQ7N79/192.168.1.34 to 172.19.0.12:9000 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=172.19.0.12/172.19.0.12:9000]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy21.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy22.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:646)
	at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:683)
	at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:655)
	at com.example.spark.KafkaHdfsSparkConsumer.analyzeBrand(KafkaHdfsSparkConsumer.java:52)
	at com.example.spark.KafkaHdfsSparkConsumer.main(KafkaHdfsSparkConsumer.java:39)
Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=172.19.0.12/172.19.0.12:9000]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:589)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
	... 32 common frames omitted
2025-09-26 00:27:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 0 time(s); maxRetries=45
2025-09-26 00:28:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 1 time(s); maxRetries=45
2025-09-26 00:28:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 2 time(s); maxRetries=45
2025-09-26 00:28:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 3 time(s); maxRetries=45
2025-09-26 00:29:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 4 time(s); maxRetries=45
2025-09-26 00:29:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 5 time(s); maxRetries=45
2025-09-26 00:29:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 6 time(s); maxRetries=45
2025-09-26 00:30:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 7 time(s); maxRetries=45
2025-09-26 00:30:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 8 time(s); maxRetries=45
2025-09-26 00:30:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 9 time(s); maxRetries=45
2025-09-26 00:31:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 10 time(s); maxRetries=45
2025-09-26 00:31:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 11 time(s); maxRetries=45
2025-09-26 00:31:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 12 time(s); maxRetries=45
2025-09-26 00:32:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 13 time(s); maxRetries=45
2025-09-26 00:32:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 14 time(s); maxRetries=45
2025-09-26 00:32:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 15 time(s); maxRetries=45
2025-09-26 00:33:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 16 time(s); maxRetries=45
2025-09-26 00:33:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 17 time(s); maxRetries=45
2025-09-26 00:33:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 18 time(s); maxRetries=45
2025-09-26 00:34:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 19 time(s); maxRetries=45
2025-09-26 00:34:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 20 time(s); maxRetries=45
2025-09-26 00:34:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 21 time(s); maxRetries=45
2025-09-26 00:35:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 22 time(s); maxRetries=45
2025-09-26 00:35:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 23 time(s); maxRetries=45
2025-09-26 00:35:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 24 time(s); maxRetries=45
2025-09-26 00:36:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 25 time(s); maxRetries=45
2025-09-26 00:36:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 26 time(s); maxRetries=45
2025-09-26 00:36:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 27 time(s); maxRetries=45
2025-09-26 00:37:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 28 time(s); maxRetries=45
2025-09-26 00:37:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 29 time(s); maxRetries=45
2025-09-26 00:37:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 30 time(s); maxRetries=45
2025-09-26 00:38:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 31 time(s); maxRetries=45
2025-09-26 00:38:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 32 time(s); maxRetries=45
2025-09-26 00:38:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 33 time(s); maxRetries=45
2025-09-26 00:39:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 34 time(s); maxRetries=45
2025-09-26 00:39:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 35 time(s); maxRetries=45
2025-09-26 00:39:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 36 time(s); maxRetries=45
2025-09-26 00:40:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 37 time(s); maxRetries=45
2025-09-26 00:40:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 38 time(s); maxRetries=45
2025-09-26 00:40:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 39 time(s); maxRetries=45
2025-09-26 00:41:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 40 time(s); maxRetries=45
2025-09-26 00:41:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 41 time(s); maxRetries=45
2025-09-26 00:41:51 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 42 time(s); maxRetries=45
2025-09-26 00:42:11 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 43 time(s); maxRetries=45
2025-09-26 00:42:31 [checkPathsExist-ForkJoinPool-2-worker-115] INFO  org.apache.hadoop.ipc.Client - Retrying connect to server: 172.19.0.12/172.19.0.12:9000. Already tried 44 time(s); maxRetries=45
2025-09-26 00:42:51 [main] INFO  org.apache.spark.SparkContext - SparkContext is stopping with exitCode 0.
2025-09-26 00:42:51 [main] INFO  o.s.jetty.server.AbstractConnector - Stopped Spark@53ec2968{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-09-26 00:42:51 [main] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://DESKTOP-GVQ7N79:4040
2025-09-26 00:42:51 [dispatcher-event-loop-0] INFO  o.a.s.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
2025-09-26 00:42:51 [main] INFO  o.a.spark.storage.memory.MemoryStore - MemoryStore cleared
2025-09-26 00:42:51 [main] INFO  o.apache.spark.storage.BlockManager - BlockManager stopped
2025-09-26 00:42:51 [main] INFO  o.a.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
2025-09-26 00:42:51 [dispatcher-event-loop-3] INFO  o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
2025-09-26 00:42:51 [main] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext
2025-09-26 00:42:51 [shutdown-hook-0] INFO  o.a.spark.util.ShutdownHookManager - Shutdown hook called
2025-09-26 00:42:51 [shutdown-hook-0] INFO  o.a.spark.util.ShutdownHookManager - Deleting directory C:\Users\Даша\AppData\Local\Temp\spark-85a75771-2936-4d3b-81dd-ea90f424aea8
